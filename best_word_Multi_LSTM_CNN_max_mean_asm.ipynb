{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, TimeDistributed, Dense, Concatenate, Dropout, BatchNormalization,GRU,LSTM,Conv1D,MaxPool1D,Flatten,Lambda,merge\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读入词向量文件\n",
    "def embed_dict(file):\n",
    "    temp={}\n",
    "    with open(file) as f:\n",
    "        for line in f.readlines():\n",
    "            s = line.strip('\\n').split(' ')\n",
    "            temp[s[0]] = [float(v) for v in s[1:]]\n",
    "    return temp\n",
    "#读入train和test\n",
    "def read_data(typein,data):\n",
    "    data = pd.merge(data,que[['qid','words']],left_on='q1',right_on='qid',how='left')\n",
    "    data = pd.merge(data,que[['qid','words']],left_on='q2',right_on='qid',how='left')\n",
    "    data.drop(['qid_x','qid_y'],axis=1,inplace=True)\n",
    "    if typein =='train':\n",
    "        columns = ['label','q1','q2','word1','word2']\n",
    "    else:\n",
    "        columns = ['q1','q2','word1','word2']\n",
    "    data.columns = columns\n",
    "    return data\n",
    "#texts_to_sequences\n",
    "def text2seq(q1,q2,MSL=25):\n",
    "    return pad_sequences(tokenizer.texts_to_sequences(q1),maxlen=MSL),pad_sequences(tokenizer.texts_to_sequences(q2),maxlen=MSL)\n",
    "#构建embedding矩阵\n",
    "def embedding_matrix(w_inx,w_dict):\n",
    "    word_embedding_matrix = np.zeros((MAX_NB_WORDS + 1, EMBEDDING_DIM))\n",
    "    for word, i in w_inx.items():\n",
    "        if i > MAX_NB_WORDS:\n",
    "            continue\n",
    "        embedding_vector = w_dict.get(str(word).upper())\n",
    "        if embedding_vector is not None:\n",
    "            word_embedding_matrix[i] = embedding_vector\n",
    "    return word_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "que = pd.read_csv('../data/question.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#全局变量\n",
    "MAX_NB_WORDS = 10000\n",
    "EMBEDDING_DIM = 300\n",
    "#######################\n",
    "test = read_data('test',test)\n",
    "word_dict = embed_dict('../data/word_embed.txt')\n",
    "train = read_data('train',train)\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(que['words'])\n",
    "word_index = tokenizer.word_index\n",
    "q1_data_tr,q2_data_tr = text2seq(train['word1'],train['word2'])\n",
    "q1_data_te,q2_data_te = text2seq(test['word1'],test['word2'])\n",
    "q_concat = np.stack([q1_data_tr,q2_data_tr],axis=1)\n",
    "word_embedding_matrix = embedding_matrix(word_index,word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_layer(q,lstm1,lstm2):\n",
    "    q = lstm_layer_1(q)\n",
    "    q = Dropout(0.3)(q)\n",
    "    q = lstm_layer_2(q)\n",
    "    q = Lambda(lambda x:K.reshape(x,(-1,25,256)))(q)\n",
    "    return q\n",
    "\n",
    "def conv_pool(conv_unit,q):\n",
    "    q_conv = conv(q)\n",
    "    q_maxp = MaxPool1D(pool_size=25)(q_conv)\n",
    "    q_maxp = Lambda(lambda x:K.reshape(x,(-1,int(x.shape[-1]))))(q_maxp)\n",
    "    q_meanp = Lambda(lambda x:K.mean(x,axis=1))(q_conv)\n",
    "    q_minp = Lambda(lambda x:K.min(x,axis=1))(q_conv)\n",
    "    return q_maxp,q_meanp,q_minp\n",
    "\n",
    "def mix_layer(q1_maxp,q1_meanp,q2_maxp,q2_meanp,q1_min,q2_min):\n",
    "    add_q_max = Lambda(lambda x:x[0] + x[1])([q1_maxp,q2_maxp])\n",
    "    sub_q_max = Lambda(lambda x:K.abs(x[0] - x[1]))([q1_maxp,q2_maxp])\n",
    "    mul_q_max = merge([q1_maxp,q2_maxp],mode='mul')\n",
    "    square_max = Lambda(lambda x:K.square(x[0] - x[1]))([q1_maxp,q2_maxp])\n",
    "    \n",
    "    add_q_mean = Lambda(lambda x:x[0] + x[1])([q1_meanp,q2_meanp])\n",
    "    sub_q_mean = Lambda(lambda x:K.abs(x[0] - x[1]))([q1_meanp,q2_meanp])\n",
    "    mul_q_mean = merge([q1_meanp,q2_meanp],mode='mul')\n",
    "    square_mean = Lambda(lambda x:K.square(x[0] - x[1]))([q1_meanp,q2_meanp])\n",
    "    \n",
    "    add_q_min = Lambda(lambda x:x[0] + x[1])([q1_min,q2_min])\n",
    "    sub_q_min = Lambda(lambda x:K.abs(x[0] - x[1]))([q1_min,q2_min])\n",
    "    mul_q_min = merge([q1_min,q2_min],mode='mul')\n",
    "    square_min = Lambda(lambda x:K.square(x[0] - x[1]))([q1_min,q2_min])\n",
    "    \n",
    "    return Concatenate()([q1_maxp,q2_maxp,add_q_max,sub_q_max,mul_q_max,square_max,\n",
    "                         q1_meanp,q2_meanp,add_q_mean,sub_q_mean,mul_q_mean,square_mean,\n",
    "                         add_q_min,sub_q_min,mul_q_min,square_min])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 228946 samples, validate on 25440 samples\n",
      "Epoch 1/50\n",
      "228946/228946 [==============================] - 61s 267us/step - loss: 0.3602 - acc: 0.8396 - val_loss: 0.2693 - val_acc: 0.8926\n",
      "Epoch 2/50\n",
      "228946/228946 [==============================] - 56s 245us/step - loss: 0.2345 - acc: 0.9015 - val_loss: 0.2259 - val_acc: 0.9094\n",
      "Epoch 3/50\n",
      "228946/228946 [==============================] - 56s 245us/step - loss: 0.1959 - acc: 0.9190 - val_loss: 0.2136 - val_acc: 0.9153\n",
      "Epoch 4/50\n",
      "228946/228946 [==============================] - 56s 246us/step - loss: 0.1716 - acc: 0.9296 - val_loss: 0.2013 - val_acc: 0.9200\n",
      "Epoch 5/50\n",
      "228946/228946 [==============================] - 56s 243us/step - loss: 0.1535 - acc: 0.9378 - val_loss: 0.1940 - val_acc: 0.9244\n",
      "Epoch 6/50\n",
      "228946/228946 [==============================] - 56s 243us/step - loss: 0.1385 - acc: 0.9440 - val_loss: 0.1941 - val_acc: 0.9245\n",
      "Epoch 7/50\n",
      "228946/228946 [==============================] - 56s 244us/step - loss: 0.1292 - acc: 0.9479 - val_loss: 0.2113 - val_acc: 0.9219\n",
      "Epoch 8/50\n",
      "228946/228946 [==============================] - 57s 248us/step - loss: 0.1190 - acc: 0.9520 - val_loss: 0.2097 - val_acc: 0.9241\n",
      "Epoch 9/50\n",
      "228946/228946 [==============================] - 56s 246us/step - loss: 0.1111 - acc: 0.9555 - val_loss: 0.2084 - val_acc: 0.9267\n",
      "Epoch 10/50\n",
      "228946/228946 [==============================] - 56s 245us/step - loss: 0.1051 - acc: 0.9577 - val_loss: 0.2106 - val_acc: 0.9268\n",
      "Train on 228946 samples, validate on 25440 samples\n",
      "Epoch 1/50\n",
      "228946/228946 [==============================] - 60s 262us/step - loss: 0.3589 - acc: 0.8390 - val_loss: 0.2974 - val_acc: 0.8811\n",
      "Epoch 2/50\n",
      "228946/228946 [==============================] - 57s 248us/step - loss: 0.2362 - acc: 0.9012 - val_loss: 0.2189 - val_acc: 0.9092\n",
      "Epoch 3/50\n",
      "228946/228946 [==============================] - 57s 248us/step - loss: 0.1976 - acc: 0.9181 - val_loss: 0.2067 - val_acc: 0.9156\n",
      "Epoch 4/50\n",
      "228946/228946 [==============================] - 56s 245us/step - loss: 0.1722 - acc: 0.9290 - val_loss: 0.1967 - val_acc: 0.9222\n",
      "Epoch 5/50\n",
      "228946/228946 [==============================] - 56s 245us/step - loss: 0.1540 - acc: 0.9371 - val_loss: 0.2008 - val_acc: 0.9226\n",
      "Epoch 6/50\n",
      "228946/228946 [==============================] - 56s 246us/step - loss: 0.1412 - acc: 0.9428 - val_loss: 0.1940 - val_acc: 0.9261\n",
      "Epoch 7/50\n",
      "228946/228946 [==============================] - 56s 246us/step - loss: 0.1301 - acc: 0.9478 - val_loss: 0.1990 - val_acc: 0.9246\n",
      "Epoch 8/50\n",
      "228946/228946 [==============================] - 56s 245us/step - loss: 0.1194 - acc: 0.9521 - val_loss: 0.1967 - val_acc: 0.9258\n",
      "Epoch 9/50\n",
      "228946/228946 [==============================] - 57s 247us/step - loss: 0.1117 - acc: 0.9551 - val_loss: 0.1991 - val_acc: 0.9270\n",
      "Epoch 10/50\n",
      "228946/228946 [==============================] - 56s 247us/step - loss: 0.1044 - acc: 0.9582 - val_loss: 0.2271 - val_acc: 0.9241\n",
      "Epoch 11/50\n",
      "228946/228946 [==============================] - 57s 249us/step - loss: 0.0993 - acc: 0.9599 - val_loss: 0.2045 - val_acc: 0.9276\n",
      "Train on 228947 samples, validate on 25439 samples\n",
      "Epoch 1/50\n",
      "228947/228947 [==============================] - 61s 268us/step - loss: 0.3619 - acc: 0.8372 - val_loss: 0.2715 - val_acc: 0.8907\n",
      "Epoch 2/50\n",
      "228947/228947 [==============================] - 56s 245us/step - loss: 0.2352 - acc: 0.9003 - val_loss: 0.2365 - val_acc: 0.9047\n",
      "Epoch 3/50\n",
      "228947/228947 [==============================] - 56s 245us/step - loss: 0.1953 - acc: 0.9194 - val_loss: 0.2109 - val_acc: 0.9128\n",
      "Epoch 4/50\n",
      "228947/228947 [==============================] - 56s 246us/step - loss: 0.1714 - acc: 0.9295 - val_loss: 0.2089 - val_acc: 0.9134\n",
      "Epoch 5/50\n",
      "228947/228947 [==============================] - 56s 246us/step - loss: 0.1542 - acc: 0.9370 - val_loss: 0.1977 - val_acc: 0.9210\n",
      "Epoch 6/50\n",
      "228947/228947 [==============================] - 56s 243us/step - loss: 0.1410 - acc: 0.9422 - val_loss: 0.1972 - val_acc: 0.9216\n",
      "Epoch 7/50\n",
      "228947/228947 [==============================] - 56s 244us/step - loss: 0.1278 - acc: 0.9478 - val_loss: 0.2100 - val_acc: 0.9237\n",
      "Epoch 8/50\n",
      "228947/228947 [==============================] - 56s 246us/step - loss: 0.1209 - acc: 0.9514 - val_loss: 0.2086 - val_acc: 0.9230\n",
      "Epoch 9/50\n",
      "228947/228947 [==============================] - 55s 242us/step - loss: 0.1119 - acc: 0.9552 - val_loss: 0.2103 - val_acc: 0.9237\n",
      "Epoch 10/50\n",
      "228947/228947 [==============================] - 56s 246us/step - loss: 0.1046 - acc: 0.9584 - val_loss: 0.2096 - val_acc: 0.9259\n",
      "Epoch 11/50\n",
      "228947/228947 [==============================] - 56s 245us/step - loss: 0.0984 - acc: 0.9608 - val_loss: 0.2082 - val_acc: 0.9247\n",
      "Train on 228947 samples, validate on 25439 samples\n",
      "Epoch 1/50\n",
      "228947/228947 [==============================] - 61s 267us/step - loss: 0.3621 - acc: 0.8387 - val_loss: 0.2657 - val_acc: 0.8873\n",
      "Epoch 2/50\n",
      "228947/228947 [==============================] - 56s 246us/step - loss: 0.2364 - acc: 0.9008 - val_loss: 0.2176 - val_acc: 0.9097\n",
      "Epoch 3/50\n",
      "228947/228947 [==============================] - 56s 245us/step - loss: 0.1970 - acc: 0.9188 - val_loss: 0.2065 - val_acc: 0.9165\n",
      "Epoch 4/50\n",
      "228947/228947 [==============================] - 57s 247us/step - loss: 0.1716 - acc: 0.9297 - val_loss: 0.1947 - val_acc: 0.9225\n",
      "Epoch 5/50\n",
      "228947/228947 [==============================] - 56s 245us/step - loss: 0.1549 - acc: 0.9362 - val_loss: 0.1931 - val_acc: 0.9259\n",
      "Epoch 6/50\n",
      "228947/228947 [==============================] - 56s 244us/step - loss: 0.1403 - acc: 0.9428 - val_loss: 0.1990 - val_acc: 0.9263\n",
      "Epoch 7/50\n",
      "228947/228947 [==============================] - 57s 250us/step - loss: 0.1291 - acc: 0.9479 - val_loss: 0.1918 - val_acc: 0.9262\n",
      "Epoch 8/50\n",
      "228947/228947 [==============================] - 58s 251us/step - loss: 0.1193 - acc: 0.9520 - val_loss: 0.2048 - val_acc: 0.9281\n",
      "Epoch 9/50\n",
      "228947/228947 [==============================] - 58s 252us/step - loss: 0.1103 - acc: 0.9556 - val_loss: 0.1994 - val_acc: 0.9290\n",
      "Epoch 10/50\n",
      "228947/228947 [==============================] - 58s 252us/step - loss: 0.1041 - acc: 0.9578 - val_loss: 0.2205 - val_acc: 0.9243\n",
      "Epoch 11/50\n",
      "228947/228947 [==============================] - 58s 253us/step - loss: 0.0973 - acc: 0.9614 - val_loss: 0.2081 - val_acc: 0.9303\n",
      "Epoch 12/50\n",
      "228947/228947 [==============================] - 57s 251us/step - loss: 0.0921 - acc: 0.9631 - val_loss: 0.2238 - val_acc: 0.9297\n",
      "Train on 228948 samples, validate on 25438 samples\n",
      "Epoch 1/50\n",
      "228948/228948 [==============================] - 63s 274us/step - loss: 0.3655 - acc: 0.8367 - val_loss: 0.2793 - val_acc: 0.8869\n",
      "Epoch 2/50\n",
      "228948/228948 [==============================] - 58s 252us/step - loss: 0.2366 - acc: 0.9004 - val_loss: 0.2346 - val_acc: 0.9057\n",
      "Epoch 3/50\n",
      "228948/228948 [==============================] - 58s 253us/step - loss: 0.1968 - acc: 0.9185 - val_loss: 0.2063 - val_acc: 0.9173\n",
      "Epoch 4/50\n",
      "228948/228948 [==============================] - 58s 252us/step - loss: 0.1710 - acc: 0.9300 - val_loss: 0.2055 - val_acc: 0.9197\n",
      "Epoch 5/50\n",
      "228948/228948 [==============================] - 58s 253us/step - loss: 0.1548 - acc: 0.9368 - val_loss: 0.1941 - val_acc: 0.9230\n",
      "Epoch 6/50\n",
      "228948/228948 [==============================] - 58s 254us/step - loss: 0.1403 - acc: 0.9433 - val_loss: 0.1938 - val_acc: 0.9269\n",
      "Epoch 7/50\n",
      "228948/228948 [==============================] - 58s 252us/step - loss: 0.1290 - acc: 0.9477 - val_loss: 0.1978 - val_acc: 0.9251\n",
      "Epoch 8/50\n",
      "228948/228948 [==============================] - 57s 251us/step - loss: 0.1201 - acc: 0.9516 - val_loss: 0.2001 - val_acc: 0.9225\n",
      "Epoch 9/50\n",
      "228948/228948 [==============================] - 57s 248us/step - loss: 0.1111 - acc: 0.9554 - val_loss: 0.2083 - val_acc: 0.9250\n",
      "Epoch 10/50\n",
      "228948/228948 [==============================] - 57s 248us/step - loss: 0.1045 - acc: 0.9581 - val_loss: 0.2010 - val_acc: 0.9270\n",
      "Epoch 11/50\n",
      "228948/228948 [==============================] - 58s 252us/step - loss: 0.0988 - acc: 0.9608 - val_loss: 0.2284 - val_acc: 0.9206\n",
      "Train on 228948 samples, validate on 25438 samples\n",
      "Epoch 1/50\n",
      "228948/228948 [==============================] - 63s 273us/step - loss: 0.3617 - acc: 0.8390 - val_loss: 0.3216 - val_acc: 0.8737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "228948/228948 [==============================] - 58s 253us/step - loss: 0.2350 - acc: 0.9014 - val_loss: 0.2320 - val_acc: 0.9060\n",
      "Epoch 3/50\n",
      "228948/228948 [==============================] - 58s 252us/step - loss: 0.1955 - acc: 0.9196 - val_loss: 0.2142 - val_acc: 0.9146\n",
      "Epoch 4/50\n",
      "228948/228948 [==============================] - 58s 252us/step - loss: 0.1706 - acc: 0.9300 - val_loss: 0.2119 - val_acc: 0.9176\n",
      "Epoch 5/50\n",
      "228948/228948 [==============================] - 57s 250us/step - loss: 0.1542 - acc: 0.9370 - val_loss: 0.2035 - val_acc: 0.9195\n",
      "Epoch 6/50\n",
      "228948/228948 [==============================] - 57s 251us/step - loss: 0.1405 - acc: 0.9430 - val_loss: 0.2022 - val_acc: 0.9231\n",
      "Epoch 7/50\n",
      "228948/228948 [==============================] - 58s 253us/step - loss: 0.1286 - acc: 0.9482 - val_loss: 0.2054 - val_acc: 0.9222\n",
      "Epoch 8/50\n",
      "228948/228948 [==============================] - 57s 251us/step - loss: 0.1196 - acc: 0.9519 - val_loss: 0.2159 - val_acc: 0.9223\n",
      "Epoch 9/50\n",
      "228948/228948 [==============================] - 57s 251us/step - loss: 0.1117 - acc: 0.9554 - val_loss: 0.2066 - val_acc: 0.9261\n",
      "Epoch 10/50\n",
      "228948/228948 [==============================] - 58s 254us/step - loss: 0.1034 - acc: 0.9589 - val_loss: 0.2259 - val_acc: 0.9209\n",
      "Epoch 11/50\n",
      "228948/228948 [==============================] - 58s 251us/step - loss: 0.0983 - acc: 0.9604 - val_loss: 0.2157 - val_acc: 0.9238\n",
      "Train on 228948 samples, validate on 25438 samples\n",
      "Epoch 1/50\n",
      "228948/228948 [==============================] - 62s 270us/step - loss: 0.3652 - acc: 0.8366 - val_loss: 0.2720 - val_acc: 0.8911\n",
      "Epoch 2/50\n",
      "228948/228948 [==============================] - 57s 247us/step - loss: 0.2362 - acc: 0.8999 - val_loss: 0.2166 - val_acc: 0.9114\n",
      "Epoch 3/50\n",
      "228948/228948 [==============================] - 57s 247us/step - loss: 0.1959 - acc: 0.9183 - val_loss: 0.2092 - val_acc: 0.9165\n",
      "Epoch 4/50\n",
      "228948/228948 [==============================] - 56s 247us/step - loss: 0.1722 - acc: 0.9288 - val_loss: 0.1968 - val_acc: 0.9220\n",
      "Epoch 5/50\n",
      "228948/228948 [==============================] - 56s 246us/step - loss: 0.1543 - acc: 0.9367 - val_loss: 0.1966 - val_acc: 0.9251\n",
      "Epoch 6/50\n",
      "228948/228948 [==============================] - 56s 244us/step - loss: 0.1408 - acc: 0.9427 - val_loss: 0.1891 - val_acc: 0.9270\n",
      "Epoch 7/50\n",
      "228948/228948 [==============================] - 56s 245us/step - loss: 0.1278 - acc: 0.9482 - val_loss: 0.1914 - val_acc: 0.9284\n",
      "Epoch 8/50\n",
      "228948/228948 [==============================] - 56s 245us/step - loss: 0.1183 - acc: 0.9528 - val_loss: 0.1913 - val_acc: 0.9285\n",
      "Epoch 9/50\n",
      "228948/228948 [==============================] - 56s 245us/step - loss: 0.1111 - acc: 0.9552 - val_loss: 0.2047 - val_acc: 0.9284\n",
      "Epoch 10/50\n",
      "228948/228948 [==============================] - 56s 244us/step - loss: 0.1041 - acc: 0.9581 - val_loss: 0.2125 - val_acc: 0.9279\n",
      "Epoch 11/50\n",
      "228948/228948 [==============================] - 56s 246us/step - loss: 0.0988 - acc: 0.9602 - val_loss: 0.2052 - val_acc: 0.9292\n",
      "Train on 228948 samples, validate on 25438 samples\n",
      "Epoch 1/50\n",
      "228948/228948 [==============================] - 63s 275us/step - loss: 0.3647 - acc: 0.8370 - val_loss: 0.2792 - val_acc: 0.8900\n",
      "Epoch 2/50\n",
      "228948/228948 [==============================] - 56s 245us/step - loss: 0.2362 - acc: 0.9003 - val_loss: 0.2342 - val_acc: 0.9076\n",
      "Epoch 3/50\n",
      "228948/228948 [==============================] - 56s 245us/step - loss: 0.1959 - acc: 0.9188 - val_loss: 0.2033 - val_acc: 0.9191\n",
      "Epoch 4/50\n",
      "228948/228948 [==============================] - 56s 244us/step - loss: 0.1719 - acc: 0.9295 - val_loss: 0.2063 - val_acc: 0.9201\n",
      "Epoch 5/50\n",
      "228948/228948 [==============================] - 57s 247us/step - loss: 0.1542 - acc: 0.9375 - val_loss: 0.2124 - val_acc: 0.9224\n",
      "Epoch 6/50\n",
      "228948/228948 [==============================] - 57s 247us/step - loss: 0.1404 - acc: 0.9427 - val_loss: 0.1916 - val_acc: 0.9253\n",
      "Epoch 7/50\n",
      "228948/228948 [==============================] - 57s 247us/step - loss: 0.1279 - acc: 0.9482 - val_loss: 0.1929 - val_acc: 0.9262\n",
      "Epoch 8/50\n",
      "228948/228948 [==============================] - 56s 245us/step - loss: 0.1189 - acc: 0.9523 - val_loss: 0.2005 - val_acc: 0.9267\n",
      "Epoch 9/50\n",
      "228948/228948 [==============================] - 57s 248us/step - loss: 0.1099 - acc: 0.9557 - val_loss: 0.2154 - val_acc: 0.9254\n",
      "Epoch 10/50\n",
      "228948/228948 [==============================] - 56s 244us/step - loss: 0.1039 - acc: 0.9582 - val_loss: 0.2097 - val_acc: 0.9276\n",
      "Epoch 11/50\n",
      "228948/228948 [==============================] - 56s 246us/step - loss: 0.0976 - acc: 0.9609 - val_loss: 0.2117 - val_acc: 0.9267\n",
      "Train on 228948 samples, validate on 25438 samples\n",
      "Epoch 1/50\n",
      "228948/228948 [==============================] - 63s 276us/step - loss: 0.3655 - acc: 0.8360 - val_loss: 0.2751 - val_acc: 0.8857\n",
      "Epoch 2/50\n",
      "228948/228948 [==============================] - 56s 244us/step - loss: 0.2373 - acc: 0.9004 - val_loss: 0.2343 - val_acc: 0.9082\n",
      "Epoch 3/50\n",
      "228948/228948 [==============================] - 56s 244us/step - loss: 0.1968 - acc: 0.9186 - val_loss: 0.2030 - val_acc: 0.9141\n",
      "Epoch 4/50\n",
      "228948/228948 [==============================] - 56s 245us/step - loss: 0.1727 - acc: 0.9291 - val_loss: 0.2030 - val_acc: 0.9200\n",
      "Epoch 5/50\n",
      "228948/228948 [==============================] - 56s 245us/step - loss: 0.1547 - acc: 0.9373 - val_loss: 0.1968 - val_acc: 0.9206\n",
      "Epoch 6/50\n",
      "228948/228948 [==============================] - 56s 246us/step - loss: 0.1411 - acc: 0.9425 - val_loss: 0.1919 - val_acc: 0.9234\n",
      "Epoch 7/50\n",
      "228948/228948 [==============================] - 57s 248us/step - loss: 0.1298 - acc: 0.9480 - val_loss: 0.1948 - val_acc: 0.9265\n",
      "Epoch 8/50\n",
      "228948/228948 [==============================] - 56s 247us/step - loss: 0.1197 - acc: 0.9517 - val_loss: 0.2040 - val_acc: 0.9244\n",
      "Epoch 9/50\n",
      "228948/228948 [==============================] - 56s 245us/step - loss: 0.1121 - acc: 0.9553 - val_loss: 0.1950 - val_acc: 0.9269\n",
      "Epoch 10/50\n",
      "228948/228948 [==============================] - 56s 245us/step - loss: 0.1046 - acc: 0.9579 - val_loss: 0.2029 - val_acc: 0.9277\n",
      "Epoch 11/50\n",
      "228948/228948 [==============================] - 56s 246us/step - loss: 0.0987 - acc: 0.9606 - val_loss: 0.2095 - val_acc: 0.9262\n",
      "Train on 228948 samples, validate on 25438 samples\n",
      "Epoch 1/50\n",
      "228948/228948 [==============================] - 64s 280us/step - loss: 0.3637 - acc: 0.8371 - val_loss: 0.2735 - val_acc: 0.8909\n",
      "Epoch 2/50\n",
      "228948/228948 [==============================] - 56s 245us/step - loss: 0.2361 - acc: 0.9003 - val_loss: 0.2203 - val_acc: 0.9081\n",
      "Epoch 3/50\n",
      "228948/228948 [==============================] - 56s 246us/step - loss: 0.1965 - acc: 0.9191 - val_loss: 0.2053 - val_acc: 0.9182\n",
      "Epoch 4/50\n",
      "228948/228948 [==============================] - 56s 246us/step - loss: 0.1727 - acc: 0.9291 - val_loss: 0.1982 - val_acc: 0.9214\n",
      "Epoch 5/50\n",
      "228948/228948 [==============================] - 57s 249us/step - loss: 0.1563 - acc: 0.9363 - val_loss: 0.1930 - val_acc: 0.9246\n",
      "Epoch 6/50\n",
      "228948/228948 [==============================] - 56s 245us/step - loss: 0.1407 - acc: 0.9428 - val_loss: 0.1964 - val_acc: 0.9252\n",
      "Epoch 7/50\n",
      "228948/228948 [==============================] - 56s 246us/step - loss: 0.1303 - acc: 0.9472 - val_loss: 0.1960 - val_acc: 0.9283\n",
      "Epoch 8/50\n",
      "228948/228948 [==============================] - 56s 247us/step - loss: 0.1211 - acc: 0.9505 - val_loss: 0.1896 - val_acc: 0.9270\n",
      "Epoch 9/50\n",
      "228948/228948 [==============================] - 56s 246us/step - loss: 0.1115 - acc: 0.9554 - val_loss: 0.2014 - val_acc: 0.9283\n",
      "Epoch 10/50\n",
      "228948/228948 [==============================] - 57s 247us/step - loss: 0.1050 - acc: 0.9582 - val_loss: 0.2009 - val_acc: 0.9291\n",
      "Epoch 11/50\n",
      "228948/228948 [==============================] - 56s 246us/step - loss: 0.0993 - acc: 0.9606 - val_loss: 0.1900 - val_acc: 0.9303\n",
      "Epoch 12/50\n",
      "228948/228948 [==============================] - 56s 245us/step - loss: 0.0936 - acc: 0.9628 - val_loss: 0.2055 - val_acc: 0.9300\n",
      "Epoch 13/50\n",
      "228948/228948 [==============================] - 57s 249us/step - loss: 0.0888 - acc: 0.9651 - val_loss: 0.2049 - val_acc: 0.9295\n"
     ]
    }
   ],
   "source": [
    "re = []\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "for i,(tr,va) in enumerate(StratifiedKFold(n_splits=10).split(q_concat,train['label'].values)):   \n",
    "    Q1_train = q_concat[tr][:,0];Q2_train = q_concat[tr][:,1]\n",
    "    Q1_test = q_concat[va][:,0];Q2_test = q_concat[va][:,1]\n",
    "    #构建embedding层，q1 和 q2共享此embedding层\n",
    "    embedding_layer = Embedding(MAX_NB_WORDS+1,EMBEDDING_DIM,weights=[word_embedding_matrix],input_length=25,trainable=False)\n",
    "    #词嵌入\n",
    "    sequence_1_input = Input(shape=(25,), dtype='int32')\n",
    "    embed_1 = embedding_layer(sequence_1_input)\n",
    "    sequence_2_input = Input(shape=(25,), dtype='int32')\n",
    "    embed_2 = embedding_layer(sequence_2_input)\n",
    "    #lstm\n",
    "    lstm_layer_1 = LSTM(256,return_sequences=True)\n",
    "    lstm_layer_2 = LSTM(256,return_sequences=True)\n",
    "    q1 = lstm_layer(embed_1,lstm_layer_1,lstm_layer_2)\n",
    "    q2 = lstm_layer(embed_2,lstm_layer_1,lstm_layer_2)\n",
    "    #用类似TextCNN的思路构建不同卷积核的特征，两个句子共用同样的卷积层\n",
    "    kernel_size = [2,3,4,5]\n",
    "    conv_concat = []\n",
    "    for kernel in kernel_size:\n",
    "        conv = Conv1D(64,kernel_size=kernel,activation='relu',padding='same')\n",
    "        q1_maxp,q1_meanp,q1_minp = conv_pool(conv,q1)\n",
    "        q2_maxp,q2_meanp,q2_minp = conv_pool(conv,q2)\n",
    "        mix = mix_layer(q1_maxp,q1_meanp,q2_maxp,q2_meanp,q1_minp,q2_minp)\n",
    "        conv_concat.append(mix)\n",
    "    conv = Concatenate()(conv_concat)\n",
    "    #全连接层\n",
    "    merged = Dropout(0.3)(conv)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dense(512, activation='relu',name='dense_output')(merged)\n",
    "    merged = Dropout(0.3)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dense(256, activation='relu',name='dense_output2')(merged)\n",
    "    merged = Dropout(0.3)(merged)\n",
    "    merged = BatchNormalization(name='bn_output')(merged)\n",
    "    preds = Dense(1, activation='sigmoid')(merged)\n",
    "    model = Model(inputs=[sequence_1_input, sequence_2_input],outputs=preds)\n",
    "    model.compile(loss='binary_crossentropy',optimizer='nadam',metrics=['acc'])\n",
    "    hist = model.fit([Q1_train, Q2_train], train['label'].values[tr],\n",
    "                 validation_data=([Q1_test, Q2_test], train['label'].values[va]),\n",
    "                 epochs=50, \n",
    "                 batch_size=1024, \n",
    "                 shuffle=True,\n",
    "                 callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001,patience=5,mode='min')])\n",
    "    pred = model.predict([q1_data_te,q2_data_te],batch_size=1024)\n",
    "    avg = [v[0] for v in pred]\n",
    "    re.append(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = np.mean(re,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission(predict_prob):\n",
    "    with open('submission_word.csv', 'w') as file:\n",
    "        file.write(str('y_pre') + '\\n')\n",
    "        for line in predict_prob:\n",
    "            file.write(str(line) + '\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submission(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
