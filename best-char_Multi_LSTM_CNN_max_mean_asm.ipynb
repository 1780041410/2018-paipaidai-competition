{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, TimeDistributed, Dense, Concatenate, Dropout, BatchNormalization,GRU,LSTM,Conv1D,MaxPool1D,Flatten,Lambda,merge\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "que = pd.read_csv('../data/question.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#读入词向量文件\n",
    "def embed_dict(file):\n",
    "    temp={}\n",
    "    with open(file) as f:\n",
    "        for line in f.readlines():\n",
    "            s = line.strip('\\n').split(' ')\n",
    "            temp[s[0]] = [float(v) for v in s[1:]]\n",
    "    return temp\n",
    "#读入train和test\n",
    "def read_data(typein,data):\n",
    "    data = pd.merge(data,que[['qid','chars']],left_on='q1',right_on='qid',how='left')\n",
    "    data = pd.merge(data,que[['qid','chars']],left_on='q2',right_on='qid',how='left')\n",
    "    data.drop(['qid_x','qid_y'],axis=1,inplace=True)\n",
    "    if typein =='train':\n",
    "        columns = ['label','q1','q2','word1','word2']\n",
    "    else:\n",
    "        columns = ['q1','q2','word1','word2']\n",
    "    data.columns = columns\n",
    "    return data\n",
    "#texts_to_sequences\n",
    "def text2seq(q1,q2,MSL=25):\n",
    "    return pad_sequences(tokenizer.texts_to_sequences(q1),maxlen=MSL),pad_sequences(tokenizer.texts_to_sequences(q2),maxlen=MSL)\n",
    "#构建embedding矩阵\n",
    "def embedding_matrix(w_inx,w_dict):\n",
    "    word_embedding_matrix = np.zeros((MAX_NB_WORDS + 1, EMBEDDING_DIM))\n",
    "    for word, i in w_inx.items():\n",
    "        if i > MAX_NB_WORDS:\n",
    "            continue\n",
    "        embedding_vector = w_dict.get(str(word).upper())\n",
    "        if embedding_vector is not None:\n",
    "            word_embedding_matrix[i] = embedding_vector\n",
    "    return word_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#全局变量\n",
    "MAX_NB_WORDS = 10000\n",
    "EMBEDDING_DIM = 300\n",
    "#######################\n",
    "test = read_data('test',test)\n",
    "word_dict = embed_dict('../data/char_embed.txt')\n",
    "train = read_data('train',train)\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(que['chars'])\n",
    "word_index = tokenizer.word_index\n",
    "q1_data_tr,q2_data_tr = text2seq(train['word1'],train['word2'])\n",
    "q1_data_te,q2_data_te = text2seq(test['word1'],test['word2'])\n",
    "q_concat = np.stack([q1_data_tr,q2_data_tr],axis=1)\n",
    "word_embedding_matrix = embedding_matrix(word_index,word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_layer(q,lstm1,lstm2):\n",
    "    q = lstm_layer_1(q)\n",
    "    q = Dropout(0.3)(q)\n",
    "    q = lstm_layer_2(q)\n",
    "    q = Lambda(lambda x:K.reshape(x,(-1,25,256)))(q)\n",
    "    return q\n",
    "\n",
    "def conv_pool(conv_unit,q):\n",
    "    q_conv = conv(q)\n",
    "    q_maxp = MaxPool1D(pool_size=25)(q_conv)\n",
    "    q_maxp = Lambda(lambda x:K.reshape(x,(-1,int(x.shape[-1]))))(q_maxp)\n",
    "    q_meanp = Lambda(lambda x:K.mean(x,axis=1))(q_conv)\n",
    "    return q_maxp,q_meanp\n",
    "\n",
    "def mix_layer(q1_maxp,q1_meanp,q2_maxp,q2_meanp):\n",
    "    add_q_max = Lambda(lambda x:x[0] + x[1])([q1_maxp,q2_maxp])\n",
    "    sub_q_max = Lambda(lambda x:K.abs(x[0] - x[1]))([q1_maxp,q2_maxp])\n",
    "    mul_q_max = merge([q1_maxp,q2_maxp],mode='mul')\n",
    "    square_max = Lambda(lambda x:K.square(x[0] - x[1]))([q1_maxp,q2_maxp])\n",
    "    \n",
    "    add_q_mean = Lambda(lambda x:x[0] + x[1])([q1_meanp,q2_meanp])\n",
    "    sub_q_mean = Lambda(lambda x:K.abs(x[0] - x[1]))([q1_meanp,q2_meanp])\n",
    "    mul_q_mean = merge([q1_meanp,q2_meanp],mode='mul')\n",
    "    square_mean = Lambda(lambda x:K.square(x[0] - x[1]))([q1_meanp,q2_meanp])\n",
    "    \n",
    "    return Concatenate()([q1_maxp,q2_maxp,add_q_max,sub_q_max,mul_q_max,square_max,\n",
    "                         q1_meanp,q2_meanp,add_q_mean,sub_q_mean,mul_q_mean,square_mean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 228946 samples, validate on 25440 samples\n",
      "Epoch 1/50\n",
      "228946/228946 [==============================] - 64s 278us/step - loss: 0.3699 - acc: 0.8351 - val_loss: 0.2890 - val_acc: 0.8856\n",
      "Epoch 2/50\n",
      "228946/228946 [==============================] - 56s 246us/step - loss: 0.2430 - acc: 0.8972 - val_loss: 0.2351 - val_acc: 0.9031\n",
      "Epoch 3/50\n",
      "228946/228946 [==============================] - 56s 244us/step - loss: 0.2044 - acc: 0.9148 - val_loss: 0.2202 - val_acc: 0.9114\n",
      "Epoch 4/50\n",
      "228946/228946 [==============================] - 56s 244us/step - loss: 0.1803 - acc: 0.9262 - val_loss: 0.2052 - val_acc: 0.9156\n",
      "Epoch 5/50\n",
      "228946/228946 [==============================] - 56s 244us/step - loss: 0.1646 - acc: 0.9326 - val_loss: 0.2005 - val_acc: 0.9208\n",
      "Epoch 6/50\n",
      "228946/228946 [==============================] - 55s 240us/step - loss: 0.1506 - acc: 0.9387 - val_loss: 0.2064 - val_acc: 0.9198\n",
      "Epoch 7/50\n",
      "228946/228946 [==============================] - 57s 249us/step - loss: 0.1415 - acc: 0.9421 - val_loss: 0.2055 - val_acc: 0.9242\n",
      "Epoch 8/50\n",
      "228946/228946 [==============================] - 58s 253us/step - loss: 0.1336 - acc: 0.9456 - val_loss: 0.1967 - val_acc: 0.9253\n",
      "Epoch 9/50\n",
      "228946/228946 [==============================] - 58s 252us/step - loss: 0.1249 - acc: 0.9493 - val_loss: 0.2009 - val_acc: 0.9237\n",
      "Epoch 10/50\n",
      "228946/228946 [==============================] - 58s 252us/step - loss: 0.1180 - acc: 0.9527 - val_loss: 0.2030 - val_acc: 0.9270\n",
      "Epoch 11/50\n",
      "228946/228946 [==============================] - 58s 252us/step - loss: 0.1139 - acc: 0.9541 - val_loss: 0.2010 - val_acc: 0.9273\n",
      "Epoch 12/50\n",
      "228946/228946 [==============================] - 58s 251us/step - loss: 0.1090 - acc: 0.9562 - val_loss: 0.2051 - val_acc: 0.9282\n",
      "Epoch 13/50\n",
      "228946/228946 [==============================] - 57s 251us/step - loss: 0.1050 - acc: 0.9578 - val_loss: 0.2183 - val_acc: 0.9259\n",
      "Train on 228946 samples, validate on 25440 samples\n",
      "Epoch 1/50\n",
      "228946/228946 [==============================] - 66s 287us/step - loss: 0.3642 - acc: 0.8361 - val_loss: 0.2723 - val_acc: 0.8877\n",
      "Epoch 2/50\n",
      "228946/228946 [==============================] - 56s 246us/step - loss: 0.2418 - acc: 0.8980 - val_loss: 0.2262 - val_acc: 0.9063\n",
      "Epoch 3/50\n",
      "228946/228946 [==============================] - 58s 252us/step - loss: 0.2034 - acc: 0.9159 - val_loss: 0.2147 - val_acc: 0.9165\n",
      "Epoch 4/50\n",
      "228946/228946 [==============================] - 58s 254us/step - loss: 0.1799 - acc: 0.9256 - val_loss: 0.1982 - val_acc: 0.9195\n",
      "Epoch 5/50\n",
      "228946/228946 [==============================] - 58s 253us/step - loss: 0.1633 - acc: 0.9328 - val_loss: 0.2011 - val_acc: 0.9226\n",
      "Epoch 6/50\n",
      "228946/228946 [==============================] - 58s 254us/step - loss: 0.1511 - acc: 0.9385 - val_loss: 0.2007 - val_acc: 0.9234\n",
      "Epoch 7/50\n",
      "228946/228946 [==============================] - 57s 250us/step - loss: 0.1406 - acc: 0.9422 - val_loss: 0.1969 - val_acc: 0.9260\n",
      "Epoch 8/50\n",
      "228946/228946 [==============================] - 58s 252us/step - loss: 0.1322 - acc: 0.9460 - val_loss: 0.1989 - val_acc: 0.9268\n",
      "Epoch 9/50\n",
      "228946/228946 [==============================] - 58s 252us/step - loss: 0.1249 - acc: 0.9496 - val_loss: 0.2031 - val_acc: 0.9256\n",
      "Epoch 10/50\n",
      "228946/228946 [==============================] - 58s 252us/step - loss: 0.1183 - acc: 0.9519 - val_loss: 0.2002 - val_acc: 0.9275\n",
      "Epoch 11/50\n",
      "228946/228946 [==============================] - 57s 251us/step - loss: 0.1142 - acc: 0.9540 - val_loss: 0.2026 - val_acc: 0.9270\n",
      "Epoch 12/50\n",
      "228946/228946 [==============================] - 58s 253us/step - loss: 0.1084 - acc: 0.9567 - val_loss: 0.2057 - val_acc: 0.9282\n",
      "Train on 228947 samples, validate on 25439 samples\n",
      "Epoch 1/50\n",
      "228947/228947 [==============================] - 65s 286us/step - loss: 0.3719 - acc: 0.8345 - val_loss: 0.3009 - val_acc: 0.8814\n",
      "Epoch 2/50\n",
      "228947/228947 [==============================] - 57s 248us/step - loss: 0.2434 - acc: 0.8978 - val_loss: 0.2432 - val_acc: 0.9028\n",
      "Epoch 3/50\n",
      "228947/228947 [==============================] - 57s 248us/step - loss: 0.2037 - acc: 0.9156 - val_loss: 0.2071 - val_acc: 0.9155\n",
      "Epoch 4/50\n",
      "228947/228947 [==============================] - 56s 246us/step - loss: 0.1793 - acc: 0.9260 - val_loss: 0.2091 - val_acc: 0.9164\n",
      "Epoch 5/50\n",
      "228947/228947 [==============================] - 57s 248us/step - loss: 0.1629 - acc: 0.9334 - val_loss: 0.2025 - val_acc: 0.9210\n",
      "Epoch 6/50\n",
      "228947/228947 [==============================] - 56s 245us/step - loss: 0.1521 - acc: 0.9377 - val_loss: 0.2009 - val_acc: 0.9206\n",
      "Epoch 7/50\n",
      "228947/228947 [==============================] - 56s 243us/step - loss: 0.1402 - acc: 0.9430 - val_loss: 0.1957 - val_acc: 0.9247\n",
      "Epoch 8/50\n",
      "228947/228947 [==============================] - 57s 251us/step - loss: 0.1333 - acc: 0.9460 - val_loss: 0.2061 - val_acc: 0.9232\n",
      "Epoch 9/50\n",
      "228947/228947 [==============================] - 58s 251us/step - loss: 0.1239 - acc: 0.9497 - val_loss: 0.2011 - val_acc: 0.9250\n",
      "Epoch 10/50\n",
      "228947/228947 [==============================] - 58s 252us/step - loss: 0.1197 - acc: 0.9520 - val_loss: 0.2027 - val_acc: 0.9285\n",
      "Epoch 11/50\n",
      "228947/228947 [==============================] - 58s 253us/step - loss: 0.1133 - acc: 0.9543 - val_loss: 0.2066 - val_acc: 0.9250\n",
      "Epoch 12/50\n",
      "228947/228947 [==============================] - 58s 252us/step - loss: 0.1093 - acc: 0.9561 - val_loss: 0.2042 - val_acc: 0.9259\n",
      "Train on 228947 samples, validate on 25439 samples\n",
      "Epoch 1/50\n",
      "228947/228947 [==============================] - 66s 289us/step - loss: 0.3662 - acc: 0.8363 - val_loss: 0.3040 - val_acc: 0.8888\n",
      "Epoch 2/50\n",
      "228947/228947 [==============================] - 58s 254us/step - loss: 0.2423 - acc: 0.8980 - val_loss: 0.2390 - val_acc: 0.9038\n",
      "Epoch 3/50\n",
      "228947/228947 [==============================] - 58s 252us/step - loss: 0.2034 - acc: 0.9158 - val_loss: 0.2139 - val_acc: 0.9123\n",
      "Epoch 4/50\n",
      "228947/228947 [==============================] - 58s 255us/step - loss: 0.1797 - acc: 0.9258 - val_loss: 0.2017 - val_acc: 0.9198\n",
      "Epoch 5/50\n",
      "228947/228947 [==============================] - 58s 253us/step - loss: 0.1637 - acc: 0.9330 - val_loss: 0.2046 - val_acc: 0.9218\n",
      "Epoch 6/50\n",
      "228947/228947 [==============================] - 58s 252us/step - loss: 0.1514 - acc: 0.9379 - val_loss: 0.1893 - val_acc: 0.9270\n",
      "Epoch 7/50\n",
      "228947/228947 [==============================] - 58s 252us/step - loss: 0.1396 - acc: 0.9432 - val_loss: 0.1916 - val_acc: 0.9243\n",
      "Epoch 8/50\n",
      "228947/228947 [==============================] - 58s 254us/step - loss: 0.1323 - acc: 0.9462 - val_loss: 0.2005 - val_acc: 0.9259\n",
      "Epoch 9/50\n",
      "228947/228947 [==============================] - 58s 255us/step - loss: 0.1235 - acc: 0.9503 - val_loss: 0.2031 - val_acc: 0.9246\n",
      "Epoch 10/50\n",
      "228947/228947 [==============================] - 58s 253us/step - loss: 0.1191 - acc: 0.9523 - val_loss: 0.2077 - val_acc: 0.9257\n",
      "Epoch 11/50\n",
      "228947/228947 [==============================] - 57s 249us/step - loss: 0.1147 - acc: 0.9534 - val_loss: 0.1934 - val_acc: 0.9278\n",
      "Train on 228948 samples, validate on 25438 samples\n",
      "Epoch 1/50\n",
      "228948/228948 [==============================] - 66s 289us/step - loss: 0.3688 - acc: 0.8348 - val_loss: 0.3015 - val_acc: 0.8771\n",
      "Epoch 2/50\n",
      "228948/228948 [==============================] - 58s 253us/step - loss: 0.2431 - acc: 0.8982 - val_loss: 0.2381 - val_acc: 0.9034\n",
      "Epoch 3/50\n",
      "228948/228948 [==============================] - 58s 253us/step - loss: 0.2053 - acc: 0.9146 - val_loss: 0.2159 - val_acc: 0.9128\n",
      "Epoch 4/50\n",
      "228948/228948 [==============================] - 58s 253us/step - loss: 0.1809 - acc: 0.9259 - val_loss: 0.2077 - val_acc: 0.9178\n",
      "Epoch 5/50\n",
      "228948/228948 [==============================] - 58s 252us/step - loss: 0.1641 - acc: 0.9327 - val_loss: 0.2024 - val_acc: 0.9226\n",
      "Epoch 6/50\n",
      "228948/228948 [==============================] - 58s 254us/step - loss: 0.1516 - acc: 0.9380 - val_loss: 0.2077 - val_acc: 0.9198\n",
      "Epoch 7/50\n",
      "228948/228948 [==============================] - 57s 250us/step - loss: 0.1403 - acc: 0.9426 - val_loss: 0.2025 - val_acc: 0.9241\n",
      "Epoch 8/50\n",
      "228948/228948 [==============================] - 58s 254us/step - loss: 0.1322 - acc: 0.9456 - val_loss: 0.2023 - val_acc: 0.9234\n",
      "Epoch 9/50\n",
      "228948/228948 [==============================] - 58s 254us/step - loss: 0.1243 - acc: 0.9499 - val_loss: 0.1947 - val_acc: 0.9271\n",
      "Epoch 10/50\n",
      "228948/228948 [==============================] - 58s 254us/step - loss: 0.1192 - acc: 0.9513 - val_loss: 0.2061 - val_acc: 0.9279\n",
      "Epoch 11/50\n",
      "228948/228948 [==============================] - 57s 251us/step - loss: 0.1136 - acc: 0.9539 - val_loss: 0.2038 - val_acc: 0.9260\n",
      "Epoch 12/50\n",
      "228948/228948 [==============================] - 58s 254us/step - loss: 0.1088 - acc: 0.9553 - val_loss: 0.2038 - val_acc: 0.9286\n",
      "Epoch 13/50\n",
      "228948/228948 [==============================] - 58s 253us/step - loss: 0.1051 - acc: 0.9576 - val_loss: 0.2049 - val_acc: 0.9288\n",
      "Epoch 14/50\n",
      "228948/228948 [==============================] - 58s 253us/step - loss: 0.0999 - acc: 0.9597 - val_loss: 0.2258 - val_acc: 0.9247\n",
      "Train on 228948 samples, validate on 25438 samples\n",
      "Epoch 1/50\n",
      "228948/228948 [==============================] - 67s 292us/step - loss: 0.3630 - acc: 0.8380 - val_loss: 0.3080 - val_acc: 0.8863\n",
      "Epoch 2/50\n",
      "228948/228948 [==============================] - 58s 253us/step - loss: 0.2411 - acc: 0.8979 - val_loss: 0.2445 - val_acc: 0.9005\n",
      "Epoch 3/50\n",
      "228948/228948 [==============================] - 58s 254us/step - loss: 0.2028 - acc: 0.9159 - val_loss: 0.2225 - val_acc: 0.9115\n",
      "Epoch 4/50\n",
      "228948/228948 [==============================] - 58s 253us/step - loss: 0.1796 - acc: 0.9260 - val_loss: 0.2112 - val_acc: 0.9147\n",
      "Epoch 5/50\n",
      "228948/228948 [==============================] - 58s 253us/step - loss: 0.1638 - acc: 0.9327 - val_loss: 0.2150 - val_acc: 0.9167\n",
      "Epoch 6/50\n",
      "228948/228948 [==============================] - 58s 254us/step - loss: 0.1492 - acc: 0.9390 - val_loss: 0.2106 - val_acc: 0.9217\n",
      "Epoch 7/50\n",
      "228948/228948 [==============================] - 58s 251us/step - loss: 0.1408 - acc: 0.9423 - val_loss: 0.2033 - val_acc: 0.9232\n",
      "Epoch 8/50\n",
      "228948/228948 [==============================] - 57s 248us/step - loss: 0.1303 - acc: 0.9466 - val_loss: 0.2022 - val_acc: 0.9220\n",
      "Epoch 9/50\n",
      "228948/228948 [==============================] - 58s 254us/step - loss: 0.1239 - acc: 0.9498 - val_loss: 0.2007 - val_acc: 0.9243\n",
      "Epoch 10/50\n",
      "228948/228948 [==============================] - 58s 254us/step - loss: 0.1190 - acc: 0.9518 - val_loss: 0.2076 - val_acc: 0.9248\n",
      "Epoch 11/50\n",
      "228948/228948 [==============================] - 58s 252us/step - loss: 0.1115 - acc: 0.9552 - val_loss: 0.2061 - val_acc: 0.9269\n",
      "Epoch 12/50\n",
      "228948/228948 [==============================] - 58s 254us/step - loss: 0.1075 - acc: 0.9566 - val_loss: 0.2079 - val_acc: 0.9267\n",
      "Epoch 13/50\n",
      "228948/228948 [==============================] - 58s 252us/step - loss: 0.1045 - acc: 0.9579 - val_loss: 0.2121 - val_acc: 0.9266\n",
      "Epoch 14/50\n",
      "228948/228948 [==============================] - 58s 253us/step - loss: 0.0991 - acc: 0.9603 - val_loss: 0.2083 - val_acc: 0.9258\n",
      "Train on 228948 samples, validate on 25438 samples\n",
      "Epoch 1/50\n",
      "228948/228948 [==============================] - 68s 296us/step - loss: 0.3629 - acc: 0.8372 - val_loss: 0.2743 - val_acc: 0.8865\n",
      "Epoch 2/50\n",
      "228948/228948 [==============================] - 58s 252us/step - loss: 0.2418 - acc: 0.8981 - val_loss: 0.2288 - val_acc: 0.9044\n",
      "Epoch 3/50\n",
      "228948/228948 [==============================] - 58s 254us/step - loss: 0.2025 - acc: 0.9155 - val_loss: 0.2081 - val_acc: 0.9154\n",
      "Epoch 4/50\n",
      "228948/228948 [==============================] - 58s 255us/step - loss: 0.1794 - acc: 0.9262 - val_loss: 0.2066 - val_acc: 0.9189\n",
      "Epoch 5/50\n",
      "228948/228948 [==============================] - 59s 256us/step - loss: 0.1639 - acc: 0.9325 - val_loss: 0.2002 - val_acc: 0.9193\n",
      "Epoch 6/50\n",
      "228948/228948 [==============================] - 58s 253us/step - loss: 0.1517 - acc: 0.9379 - val_loss: 0.1861 - val_acc: 0.9261\n",
      "Epoch 7/50\n",
      "228948/228948 [==============================] - 58s 254us/step - loss: 0.1421 - acc: 0.9418 - val_loss: 0.1945 - val_acc: 0.9233\n",
      "Epoch 8/50\n",
      "228948/228948 [==============================] - 59s 256us/step - loss: 0.1318 - acc: 0.9464 - val_loss: 0.1998 - val_acc: 0.9248\n",
      "Epoch 9/50\n",
      "228948/228948 [==============================] - 58s 253us/step - loss: 0.1255 - acc: 0.9486 - val_loss: 0.1920 - val_acc: 0.9264\n",
      "Epoch 10/50\n",
      "228948/228948 [==============================] - 59s 258us/step - loss: 0.1192 - acc: 0.9518 - val_loss: 0.1934 - val_acc: 0.9281\n",
      "Epoch 11/50\n",
      "228948/228948 [==============================] - 58s 254us/step - loss: 0.1137 - acc: 0.9538 - val_loss: 0.1948 - val_acc: 0.9262\n",
      "Train on 228948 samples, validate on 25438 samples\n",
      "Epoch 1/50\n",
      "228948/228948 [==============================] - 68s 296us/step - loss: 0.3641 - acc: 0.8378 - val_loss: 0.2941 - val_acc: 0.8878\n",
      "Epoch 2/50\n",
      "228948/228948 [==============================] - 58s 255us/step - loss: 0.2421 - acc: 0.8974 - val_loss: 0.2314 - val_acc: 0.9064\n",
      "Epoch 3/50\n",
      "228948/228948 [==============================] - 56s 246us/step - loss: 0.2029 - acc: 0.9153 - val_loss: 0.2101 - val_acc: 0.9172\n",
      "Epoch 4/50\n",
      "228948/228948 [==============================] - 58s 251us/step - loss: 0.1800 - acc: 0.9258 - val_loss: 0.2046 - val_acc: 0.9212\n",
      "Epoch 5/50\n",
      "228948/228948 [==============================] - 58s 253us/step - loss: 0.1626 - acc: 0.9328 - val_loss: 0.1996 - val_acc: 0.9248\n",
      "Epoch 6/50\n",
      "228948/228948 [==============================] - 58s 254us/step - loss: 0.1513 - acc: 0.9381 - val_loss: 0.1931 - val_acc: 0.9268\n",
      "Epoch 7/50\n",
      "228948/228948 [==============================] - 58s 253us/step - loss: 0.1403 - acc: 0.9428 - val_loss: 0.1962 - val_acc: 0.9256\n",
      "Epoch 8/50\n",
      "228948/228948 [==============================] - 57s 250us/step - loss: 0.1331 - acc: 0.9452 - val_loss: 0.1969 - val_acc: 0.9281\n",
      "Epoch 9/50\n",
      "228948/228948 [==============================] - 58s 253us/step - loss: 0.1254 - acc: 0.9490 - val_loss: 0.1993 - val_acc: 0.9301\n",
      "Epoch 10/50\n",
      "228948/228948 [==============================] - 58s 254us/step - loss: 0.1202 - acc: 0.9517 - val_loss: 0.2018 - val_acc: 0.9277\n",
      "Epoch 11/50\n",
      "228948/228948 [==============================] - 58s 254us/step - loss: 0.1154 - acc: 0.9533 - val_loss: 0.1950 - val_acc: 0.9295\n",
      "Train on 228948 samples, validate on 25438 samples\n",
      "Epoch 1/50\n",
      "228948/228948 [==============================] - 68s 298us/step - loss: 0.3680 - acc: 0.8357 - val_loss: 0.3151 - val_acc: 0.8775\n",
      "Epoch 2/50\n",
      "228948/228948 [==============================] - 58s 254us/step - loss: 0.2410 - acc: 0.8979 - val_loss: 0.2401 - val_acc: 0.9047\n",
      "Epoch 3/50\n",
      "228948/228948 [==============================] - 58s 253us/step - loss: 0.2039 - acc: 0.9149 - val_loss: 0.2151 - val_acc: 0.9145\n",
      "Epoch 4/50\n",
      "228948/228948 [==============================] - 58s 254us/step - loss: 0.1800 - acc: 0.9256 - val_loss: 0.2044 - val_acc: 0.9194\n",
      "Epoch 5/50\n",
      "228948/228948 [==============================] - 58s 252us/step - loss: 0.1632 - acc: 0.9330 - val_loss: 0.2023 - val_acc: 0.9199\n",
      "Epoch 6/50\n",
      "228948/228948 [==============================] - 58s 255us/step - loss: 0.1501 - acc: 0.9386 - val_loss: 0.2118 - val_acc: 0.9223\n",
      "Epoch 7/50\n",
      "228948/228948 [==============================] - 58s 253us/step - loss: 0.1402 - acc: 0.9431 - val_loss: 0.2019 - val_acc: 0.9217\n",
      "Epoch 8/50\n",
      "228948/228948 [==============================] - 58s 254us/step - loss: 0.1309 - acc: 0.9465 - val_loss: 0.2007 - val_acc: 0.9244\n",
      "Epoch 9/50\n",
      "228948/228948 [==============================] - 58s 253us/step - loss: 0.1245 - acc: 0.9496 - val_loss: 0.2087 - val_acc: 0.9276\n",
      "Epoch 10/50\n",
      "228948/228948 [==============================] - 58s 253us/step - loss: 0.1178 - acc: 0.9527 - val_loss: 0.1961 - val_acc: 0.9264\n",
      "Epoch 11/50\n",
      "228948/228948 [==============================] - 58s 254us/step - loss: 0.1132 - acc: 0.9541 - val_loss: 0.2123 - val_acc: 0.9255\n",
      "Epoch 12/50\n",
      "228948/228948 [==============================] - 58s 254us/step - loss: 0.1082 - acc: 0.9561 - val_loss: 0.2095 - val_acc: 0.9270\n",
      "Epoch 13/50\n",
      "228948/228948 [==============================] - 58s 252us/step - loss: 0.1034 - acc: 0.9587 - val_loss: 0.2176 - val_acc: 0.9279\n",
      "Epoch 14/50\n",
      "228948/228948 [==============================] - 58s 254us/step - loss: 0.1008 - acc: 0.9599 - val_loss: 0.2215 - val_acc: 0.9250\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228948/228948 [==============================] - 58s 254us/step - loss: 0.0974 - acc: 0.9610 - val_loss: 0.2134 - val_acc: 0.9281\n",
      "Train on 228948 samples, validate on 25438 samples\n",
      "Epoch 1/50\n",
      "228948/228948 [==============================] - 69s 299us/step - loss: 0.3676 - acc: 0.8350 - val_loss: 0.2804 - val_acc: 0.8895\n",
      "Epoch 2/50\n",
      "228948/228948 [==============================] - 58s 255us/step - loss: 0.2416 - acc: 0.8978 - val_loss: 0.2303 - val_acc: 0.8999\n",
      "Epoch 3/50\n",
      "228948/228948 [==============================] - 58s 254us/step - loss: 0.2027 - acc: 0.9156 - val_loss: 0.2082 - val_acc: 0.9155\n",
      "Epoch 4/50\n",
      "228948/228948 [==============================] - 58s 253us/step - loss: 0.1798 - acc: 0.9257 - val_loss: 0.1999 - val_acc: 0.9183\n",
      "Epoch 5/50\n",
      "228948/228948 [==============================] - 58s 252us/step - loss: 0.1626 - acc: 0.9336 - val_loss: 0.2038 - val_acc: 0.9213\n",
      "Epoch 6/50\n",
      "228948/228948 [==============================] - 58s 253us/step - loss: 0.1499 - acc: 0.9390 - val_loss: 0.1906 - val_acc: 0.9241\n",
      "Epoch 7/50\n",
      "228948/228948 [==============================] - 58s 254us/step - loss: 0.1409 - acc: 0.9429 - val_loss: 0.1888 - val_acc: 0.9259\n",
      "Epoch 8/50\n",
      "228948/228948 [==============================] - 57s 249us/step - loss: 0.1314 - acc: 0.9468 - val_loss: 0.1902 - val_acc: 0.9261\n",
      "Epoch 9/50\n",
      "228948/228948 [==============================] - 57s 248us/step - loss: 0.1249 - acc: 0.9497 - val_loss: 0.1897 - val_acc: 0.9281\n",
      "Epoch 10/50\n",
      "228948/228948 [==============================] - 57s 247us/step - loss: 0.1190 - acc: 0.9516 - val_loss: 0.1976 - val_acc: 0.9287\n",
      "Epoch 11/50\n",
      "228948/228948 [==============================] - 56s 245us/step - loss: 0.1131 - acc: 0.9543 - val_loss: 0.1948 - val_acc: 0.9283\n",
      "Epoch 12/50\n",
      "228948/228948 [==============================] - 57s 249us/step - loss: 0.1081 - acc: 0.9562 - val_loss: 0.1953 - val_acc: 0.9284\n"
     ]
    }
   ],
   "source": [
    "re = []\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "for i,(tr,va) in enumerate(StratifiedKFold(n_splits=10).split(q_concat,train['label'].values)):   \n",
    "    Q1_train = q_concat[tr][:,0];Q2_train = q_concat[tr][:,1]\n",
    "    Q1_test = q_concat[va][:,0];Q2_test = q_concat[va][:,1]\n",
    "    #构建embedding层，q1 和 q2共享此embedding层\n",
    "    embedding_layer = Embedding(MAX_NB_WORDS+1,EMBEDDING_DIM,weights=[word_embedding_matrix],input_length=25,trainable=False)\n",
    "    #词嵌入\n",
    "    sequence_1_input = Input(shape=(25,), dtype='int32')\n",
    "    embed_1 = embedding_layer(sequence_1_input)\n",
    "    sequence_2_input = Input(shape=(25,), dtype='int32')\n",
    "    embed_2 = embedding_layer(sequence_2_input)\n",
    "    #lstm\n",
    "    lstm_layer_1 = LSTM(256,return_sequences=True)\n",
    "    lstm_layer_2 = LSTM(256,return_sequences=True)\n",
    "    q1 = lstm_layer(embed_1,lstm_layer_1,lstm_layer_2)\n",
    "    q2 = lstm_layer(embed_2,lstm_layer_1,lstm_layer_2)\n",
    "    #用类似TextCNN的思路构建不同卷积核的特征，两个句子共用同样的卷积层\n",
    "    kernel_size = [2,3,4,5]\n",
    "    conv_concat = []\n",
    "    for kernel in kernel_size:\n",
    "        conv = Conv1D(64,kernel_size=kernel,activation='relu',padding='same')\n",
    "        q1_maxp,q1_meanp = conv_pool(conv,q1)\n",
    "        q2_maxp,q2_meanp = conv_pool(conv,q2)\n",
    "        mix = mix_layer(q1_maxp,q1_meanp,q2_maxp,q2_meanp)\n",
    "        conv_concat.append(mix)\n",
    "    conv = Concatenate()(conv_concat)\n",
    "    #全连接层\n",
    "    merged = Dropout(0.3)(conv)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dense(512, activation='relu',name='dense_output')(merged)\n",
    "    merged = Dropout(0.3)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dense(256, activation='relu',name='dense_output2')(merged)\n",
    "    merged = Dropout(0.3)(merged)\n",
    "    merged = BatchNormalization(name='bn_output')(merged)\n",
    "    preds = Dense(1, activation='sigmoid')(merged)\n",
    "    model = Model(inputs=[sequence_1_input, sequence_2_input],outputs=preds)\n",
    "    model.compile(loss='binary_crossentropy',optimizer='nadam',metrics=['acc'])\n",
    "    hist = model.fit([Q1_train, Q2_train], train['label'].values[tr],\n",
    "                 validation_data=([Q1_test, Q2_test], train['label'].values[va]),\n",
    "                 epochs=50, \n",
    "                 batch_size=1024, \n",
    "                 shuffle=True,\n",
    "                 callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001,patience=5,mode='min')])\n",
    "    pred = model.predict([q1_data_te,q2_data_te],batch_size=1024)\n",
    "    avg = [v[0] for v in pred]\n",
    "    re.append(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg = np.mean(re,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_submission(predict_prob):\n",
    "    with open('submission_char.csv', 'w') as file:\n",
    "        file.write(str('y_pre') + '\\n')\n",
    "        for line in predict_prob:\n",
    "            file.write(str(line) + '\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_submission(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
